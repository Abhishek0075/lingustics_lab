{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    # Positive examples\n",
    "    (\"I love this product, it's fantastic!\", \"positive\"),\n",
    "    (\"Absolutely wonderful, would buy again!\", \"positive\"),\n",
    "    (\"This item exceeded my expectations, highly recommend!\", \"positive\"),\n",
    "    (\"Great quality, fast shipping, and excellent customer service!\", \"positive\"),\n",
    "    (\"Very pleased with this purchase, works perfectly!\", \"positive\"),\n",
    "    (\"Amazing experience, I'll definitely be back for more!\", \"positive\"),\n",
    "    (\"This is my favorite product, couldn't be happier!\", \"positive\"),\n",
    "    (\"The service was outstanding, I am extremely satisfied!\", \"positive\"),\n",
    "    (\"Superb performance and reliable, worth every penny!\", \"positive\"),\n",
    "    (\"Top-notch quality and affordable, highly recommended!\", \"positive\"),\n",
    "    \n",
    "    # Negative examples\n",
    "    (\"This is terrible, I hate it\", \"negative\"),\n",
    "    (\"Worst experience, totally disappointed\", \"negative\"),\n",
    "    (\"The product broke after one use, very poor quality!\", \"negative\"),\n",
    "    (\"Extremely dissatisfied, not worth the money\", \"negative\"),\n",
    "    (\"I regret buying this, it was a waste of money\", \"negative\"),\n",
    "    (\"Poor customer service and bad quality\", \"negative\"),\n",
    "    (\"This item did not work as expected, I'm very disappointed\", \"negative\"),\n",
    "    (\"The quality is horrible, don't buy this!\", \"negative\"),\n",
    "    (\"Not what I was expecting, very low quality\", \"negative\"),\n",
    "    (\"This was a mistake, I would not recommend it to anyone\", \"negative\"),\n",
    "    \n",
    "    # Neutral examples\n",
    "    (\"It's okay, not the best but decent\", \"neutral\"),\n",
    "    (\"This product is average, does the job\", \"neutral\"),\n",
    "    (\"Nothing special, but it works as expected\", \"neutral\"),\n",
    "    (\"The quality is fine, nothing extraordinary\", \"neutral\"),\n",
    "    (\"It's a functional product, meets basic requirements\", \"neutral\"),\n",
    "    (\"This is a standard item, performs as described\", \"neutral\"),\n",
    "    (\"Average experience, could be better\", \"neutral\"),\n",
    "    (\"The product is fair for the price\", \"neutral\"),\n",
    "    (\"Not too bad, but not impressive either\", \"neutral\"),\n",
    "    (\"It's neither great nor terrible, just okay\", \"neutral\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "\n",
    "# Build vocabulary and word counts for each class\n",
    "def build_word_counts(X_train, y_train):\n",
    "    class_word_counts = defaultdict(Counter)\n",
    "    class_doc_counts = Counter(y_train)\n",
    "    vocabulary = set()\n",
    "\n",
    "    for text, label in zip(X_train, y_train):\n",
    "        words = tokenize(text)\n",
    "        vocabulary.update(words)\n",
    "        class_word_counts[label].update(words)\n",
    "    \n",
    "    return class_word_counts, class_doc_counts, vocabulary\n",
    "\n",
    "\n",
    "def calculate_class_probabilities(class_word_counts, class_doc_counts, vocabulary, k_smoothing=1):\n",
    "    class_probabilities = {}\n",
    "    word_probabilities = {label: {} for label in class_doc_counts}\n",
    "\n",
    "    total_docs = sum(class_doc_counts.values())\n",
    "    for label in class_doc_counts:\n",
    "        class_probabilities[label] = np.log(class_doc_counts[label] / total_docs)  # log(P(label))\n",
    "        total_words_in_class = sum(class_word_counts[label].values())\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            word_count = class_word_counts[label][word]\n",
    "            # Add-k smoothing: P(word|label) = (count + k_smoothing) / (total words + k_smoothing * vocab_size)\n",
    "            word_probabilities[label][word] = np.log((word_count + k_smoothing) / (total_words_in_class + k_smoothing * len(vocabulary)))\n",
    "    \n",
    "    return class_probabilities, word_probabilities\n",
    "\n",
    "def predict(text, class_probabilities, word_probabilities, vocabulary):\n",
    "    words = tokenize(text)\n",
    "    scores = {}\n",
    "    \n",
    "    for label in class_probabilities:\n",
    "        # Start with log(P(label))\n",
    "        score = class_probabilities[label]\n",
    "        \n",
    "        # Add log(P(word|label)) for each word in the input text\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                score += word_probabilities[label].get(word, np.log(1 / (len(vocabulary) + 1)))\n",
    "        \n",
    "        scores[label] = score\n",
    "\n",
    "    return max(scores, key=scores.get)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k=0.25: 0.6666666666666666\n",
      "Accuracy for k=0.75: 0.6666666666666666\n",
      "Accuracy for k=1: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "texts, labels = zip(*data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "class_word_counts, class_doc_counts, vocabulary = build_word_counts(X_train, y_train)\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "for k in [0.25, 0.75, 1]:\n",
    "    class_probabilities, word_probabilities = calculate_class_probabilities(class_word_counts, class_doc_counts, vocabulary, k_smoothing=k)\n",
    "    y_pred = [predict(text, class_probabilities, word_probabilities, vocabulary) for text in X_test]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy for k={k}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Okay, wonderful\", class_probabilities, word_probabilities, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
