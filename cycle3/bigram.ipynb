{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Probabilities: {('this', 'is'): 0.2857142857142857, ('is', 'a'): 0.42857142857142855, ('a', 'sample'): 0.42857142857142855, ('sample', 'text'): 0.2857142857142857, ('text', 'this'): 0.2857142857142857, ('this', 'text'): 0.2857142857142857, ('text', 'is'): 0.2857142857142857}\n",
      "Probability of the sentence 'this is a sample': 0.05247813411078719\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text: lowercase and tokenize.\"\"\"\n",
    "    # Convert to lowercase and split into words\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_bigrams(corpus):\n",
    "    \"\"\"Build bigram counts and unigram counts from the corpus.\"\"\"\n",
    "    # Tokenize the corpus\n",
    "    tokens = preprocess_text(corpus)\n",
    "\n",
    "    # Count unigrams and bigrams\n",
    "    unigram_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(zip(tokens[:-1], tokens[1:])) # Zips list of tokens from (first to second last) - (second to last)\n",
    "\n",
    "    return unigram_counts, bigram_counts\n",
    "\n",
    "def calculate_bigram_probabilities(unigram_counts, bigram_counts):\n",
    "    \"\"\"Calculate bigram probabilities with Laplace smoothing.\"\"\"\n",
    "    vocab_size = len(unigram_counts)\n",
    "    bigram_probabilities = {}\n",
    "\n",
    "    for (w1, w2), count in bigram_counts.items():\n",
    "        # Apply Laplace smoothing: (count + 1) / (unigram_count + vocab_size)\n",
    "        bigram_probabilities[(w1, w2)] = (count + 1) / (unigram_counts[w1] + vocab_size)\n",
    "\n",
    "    return bigram_probabilities, vocab_size\n",
    "\n",
    "def calculate_sentence_probability(sentence, unigram_counts, bigram_probabilities, vocab_size):\n",
    "    \"\"\"Calculate the probability of a sentence using bigram probabilities.\"\"\"\n",
    "    tokens = preprocess_text(sentence)\n",
    "    probability = 0.0\n",
    "\n",
    "    for i in range(len(tokens) - 1):\n",
    "        w1, w2 = tokens[i], tokens[i + 1]\n",
    "        if (w1, w2) in bigram_probabilities:\n",
    "            prob = bigram_probabilities[(w1, w2)]\n",
    "        else:\n",
    "            # Handle unseen bigrams with Laplace smoothing\n",
    "            prob = 1 / (unigram_counts.get(w1, 0) + vocab_size)\n",
    "\n",
    "        # Use log probability to avoid underflow\n",
    "        probability += math.log(prob)\n",
    "\n",
    "    return math.exp(probability)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = \"this is a sample text this text is a sample\"\n",
    "    sentence = \"this is a sample\"\n",
    "\n",
    "    unigram_counts, bigram_counts = build_bigrams(corpus)\n",
    "    bigram_probabilities, vocab_size = calculate_bigram_probabilities(unigram_counts, bigram_counts)\n",
    "    \n",
    "    sentence_probability = calculate_sentence_probability(sentence, unigram_counts, bigram_probabilities, vocab_size)\n",
    "\n",
    "    print(\"Bigram Probabilities:\", bigram_probabilities)\n",
    "    print(\"Probability of the sentence '{}':\".format(sentence), sentence_probability)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
